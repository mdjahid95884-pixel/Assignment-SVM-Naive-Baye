{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Classification: Decision ,Trees,\n",
        "# SVM, and Naive Bayes\n",
        "# Assignment\n"
      ],
      "metadata": {
        "id": "EAv_jdSx6tzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 : What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "**Answer:** Information Gain tells us how good a feature is at splitting the data in a decision tree. It measures how much uncertainty (impurity) is reduced after a split.\n",
        "\n",
        "In simple words:\n",
        "\n",
        "First, we calculate how mixed the data is (using entropy).\n",
        "\n",
        "Then we split the data using a feature.\n",
        "\n",
        "Information Gain = Impurity before split − Impurity after split.\n",
        "\n",
        "The feature with the highest Information Gain is chosen to make the split in the decision tree."
      ],
      "metadata": {
        "id": "_5uwWAFq7A-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "KilVgOcE97Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "**Hint: Directly compares the two main impurity measures, highlighting strengths,weaknesses, and appropriate use cases.**\n",
        "\n",
        "**Answer:** Both Gini Impurity and Entropy are used to measure how mixed the classes are in a node.\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        "Faster to calculate\n",
        "\n",
        "Used by default in CART decision trees\n",
        "\n",
        "Formula: Gini = 1 − Σ(p²)\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "Based on information theory\n",
        "\n",
        "Slightly slower\n",
        "\n",
        "Formula: Entropy = −Σ(p log₂ p)\n",
        "\n",
        "**Difference in short:** Gini is faster and simpler Entropy is more theoretical Both usually give similar results"
      ],
      "metadata": {
        "id": "0uTXr5_B8Dhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "5eDbJBjP96LU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "**Answer:** Pre-pruning means stopping the growth of a decision tree early to avoid overfitting.\n",
        "\n",
        "Examples of pre-pruning rules:\n",
        "\n",
        "Set maximum depth of the tree\n",
        "\n",
        "Minimum samples required to split\n",
        "\n",
        "Minimum samples in a leaf node\n",
        "\n",
        "**It helps the model generalize better on new data.**\n"
      ],
      "metadata": {
        "id": "vHdpH3KL8vxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "MjWk1JHo92fN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).**\n",
        "\n",
        "**Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:** ⬇️"
      ],
      "metadata": {
        "id": "oRh6KqgS9B1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBHRZTUM6b_r",
        "outputId": "0fa13825-e59e-412b-9041-468d35bad029"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "# Train Decision Tree with Gini\n",
        "dt = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt.fit(X, y)\n",
        "\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", dt.feature_importances_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "MuDqguvg-X38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "**Answer:** SVM is a supervised machine learning algorithm used for classification and regression.\n",
        "\n",
        "**It works by:**\n",
        "\n",
        "Finding a line or boundary (called a hyperplane)\n",
        "\n",
        "Maximizing the distance between different classes\n",
        "\n",
        "SVM focuses on the most important data points called support vectors."
      ],
      "metadata": {
        "id": "dAByNnkC90QV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "jbmxDflF-alE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: What is the Kernel Trick in SVM?**\n",
        "\n",
        "**Answer:** The kernel trick helps SVM solve non-linear problems.\n",
        "\n",
        "It converts data into a higher dimension so that a straight line can separate the classes.\n",
        "\n",
        "**Common kernels:**\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "RBF (Gaussian)\n"
      ],
      "metadata": {
        "id": "fjoxeNlh-bGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "F5ho7OcN-oZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.**\n",
        "\n",
        "**Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.**\n",
        "\n",
        "**Answer:** ⬇️"
      ],
      "metadata": {
        "id": "430cBgk1-o2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Linear SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, linear_svm.predict(X_test))\n",
        "\n",
        "\n",
        "# RBF SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, rbf_svm.predict(X_test))\n",
        "\n",
        "\n",
        "print(\"Linear SVM Accuracy:\", linear_acc)\n",
        "print(\"RBF SVM Accuracy:\", rbf_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9TvIlGs-67V",
        "outputId": "419a5497-984c-40c4-d14e-d607391c2f33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 1.0\n",
            "RBF SVM Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "nOAjBw1M_C52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "**Answer:** Naive Bayes is a probabilistic classification algorithm based on Bayes’ Theorem.\n",
        "\n",
        "It is called Naïve because it assumes all features are independent of each other, which is rarely true in real life.\n",
        "\n",
        "Despite this assumption, it works very well for many problems like spam detection.\n"
      ],
      "metadata": {
        "id": "nUtzuD7K_Det"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "ygbCvuzD_Sy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "\n",
        "**Answer:** **Gaussian Naïve Bayes:**\n",
        "\n",
        "Used for continuous data\n",
        "\n",
        "Assumes data follows normal distribution\n",
        "\n",
        "**Multinomial Naïve Bayes:**\n",
        "\n",
        "Used for count data\n",
        "\n",
        "Common in text classification\n",
        "\n",
        "Bernoulli Naïve Bayes:\n",
        "\n",
        "Used for binary data (0/1)\n",
        "\n",
        "Works well with presence/absence of features"
      ],
      "metadata": {
        "id": "vnKfbelc_TO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "RYy_nUlb_oI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: Breast Cancer Dataset**\n",
        "\n",
        "**Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.**\n",
        "\n",
        "**Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)**\n",
        "\n",
        "**Answer:** ⬇️"
      ],
      "metadata": {
        "id": "TY-4p0h2_o6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Train model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPrcVxJj__NI",
        "outputId": "f2744d67-651e-46e1-f86d-21e24bb792bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}